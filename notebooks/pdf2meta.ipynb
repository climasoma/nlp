{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a1a82d-11fa-4732-af71-6629405365a6",
   "metadata": {},
   "source": [
    "# Automatic extraction from pdf\n",
    "\n",
    "This tool uses regular expressions to extract soil science information from pdf. Note that this tool is not able to extract information from pdf tables, only from text.\n",
    "\n",
    "This tool is experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3be7bac-bd9e-4b69-96b6-f06c53c792e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#import spacy\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import json\n",
    "import time\n",
    "from ipyleaflet import Map, Marker\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0) # and then each column will be just as big as it needs to be\n",
    "\n",
    "datadir = '../data/kunsat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe7f6f6-981b-453b-b978-405577935201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text from pdf\n",
    "def pdf2text(fo):\n",
    "    text = ''\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        fname = os.path.join(td, 'fname.pdf')\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(fo)\n",
    "        os.system('pdftotext ' + fname)\n",
    "        fpath = fname[:-4] + '.txt'\n",
    "        if os.path.exists(fpath):\n",
    "            with open(fpath, 'r', encoding='utf8') as f:\n",
    "                try:\n",
    "                    text = f.read()\n",
    "                    if len(text) < 100:\n",
    "                        print('pdf2text: document too small')\n",
    "                except:\n",
    "                    print('pdf2text: error with')\n",
    "    return text\n",
    "\n",
    "# test\n",
    "# with open('../data/from-kunsat/papers/alagna2016.pdf', 'rb') as f:\n",
    "#     fo = f.read()\n",
    "# text = pdf2text(fo)\n",
    "# len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f2b68a-fe53-4734-a350-9339e607cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract abstract\n",
    "def getAbstract(doc):\n",
    "    # detect abstract, keywords, reference, title - overwrite rawdocs!\n",
    "    abstract = ''\n",
    "    body = ''\n",
    "    code = ''  # determine how the abstract was found\n",
    "    splitdoc = re.split('a b s t r a c t|abstract|ABSTRACT|Abstract|A B S T R A C T|Summary|s u m m a r y|SUMMARY', doc)\n",
    "    foundStart = True\n",
    "    if len(splitdoc) == 1:\n",
    "        foundStart = False\n",
    "    text = '\\n'.join(splitdoc[1:]) if len(splitdoc) > 1 else splitdoc[0] # if not keyword abstract found, take all\n",
    "\n",
    "    # assuming first paragraph is the abstract\n",
    "    splitEnd = re.split('Published|Introduction|Copyright|©|Keywords|keywords|KEYWORDS|KEY WORDS|Citation', text)\n",
    "    foundEnd = True\n",
    "    if len(splitEnd) == 1:\n",
    "        foundEnd = False\n",
    "        #print('abstract end not found', fname)\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        ps = []\n",
    "        a = ''\n",
    "        # we only want paragraphs split that are more than 10 characters\n",
    "        for p in paragraphs:\n",
    "            a = a + '\\n\\n' + p\n",
    "            if len(a) > 10:\n",
    "                ps.append(a)\n",
    "                a = ''\n",
    "        if foundStart is True:\n",
    "            abstract = ps[0]\n",
    "            code = 'ps[0]'\n",
    "        else:\n",
    "            # without detection of the start or end, we\n",
    "            # blindly assume that abstract is first paragraph\n",
    "            abstract = ps[1]\n",
    "            code = 'first'\n",
    "            print('abstract = first paragraph', fname)\n",
    "    else:\n",
    "        abstract = ''\n",
    "        for j, s in enumerate(splitEnd):\n",
    "            if len(s) > 50 and abstract == '':\n",
    "                abstract = s\n",
    "                code = 's{:d}'.format(j)\n",
    "\n",
    "    # cleaning up the abstract\n",
    "    if abstract[0] in [':', '.']:\n",
    "        abstract = abstract[1:]\n",
    "    abstract = abstract.strip()\n",
    "\n",
    "    # edge case (if we have two first large paragraphs)\n",
    "    ts = abstract.split('\\n\\n')\n",
    "    if len(ts) > 1:\n",
    "        if (len(ts[0]) > 800) & (len(ts[1]) > 800):\n",
    "            if ts[0][0] in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "                abstract = ts[0]\n",
    "                #print(fname, 'first')\n",
    "                code = 'ts[0]'\n",
    "            else:\n",
    "                abstract = ts[1]\n",
    "                #print(fname, 'second')\n",
    "                code = 'ts[1]'\n",
    "\n",
    "    # remove the abstract from the body\n",
    "    body = text.replace(abstract, '')\n",
    "\n",
    "    # remove the references\n",
    "    #if len(re.findall('r(é|e)f(é|e)rences?|r ?e ?f ?e ?r ?e ?n ?c ?e ?s?', body, flags=re.IGNORECASE)) == 0:\n",
    "    #    print('no ref found for', fname)\n",
    "    parts = re.split('\\n\\s?r(é|e)f(é|e)rences?\\n|\\n\\s?r ?e ?f ?e ?r ?e ?n ?c ?e ?s?\\n', body, flags=re.IGNORECASE)\n",
    "    if len(parts) > 2:  # at least one 'reference' found\n",
    "        body = '\\nReferences\\n'.join([a for a in parts[:-1] if a is not None])\n",
    "    else:\n",
    "        body = body\n",
    "        #print('ref not found for', dfpub.loc[i, 'fname'])\n",
    "        # for old papers with no ref section parsed to OCR, that's often the case\n",
    "    # failsafe for badly detected abstract\n",
    "    ratio = len(body) / len(text)\n",
    "    if ratio < 0.6:\n",
    "        print('getAbstract: abstract probably not well detected for', fname, '(ratio: {:.2f})'.format(ratio))\n",
    "        body = text\n",
    "    return body, abstract\n",
    "\n",
    "# test\n",
    "# doc, abstract = getAbstract(text)\n",
    "# len(doc), len(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a0c705f-b12c-46d2-a7e7-1dd8288362f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract soil type\n",
    "\n",
    "# load soils from keyword trees\n",
    "with open(datadir + '../keyword-trees/SoilTypeFAO.json', 'r') as f:\n",
    "    soilTypeFAO = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeQualifiersFAO.json', 'r') as f:\n",
    "    soilTypeQualifiersFAO = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeGER.json', 'r') as f:\n",
    "    soilTypeGER = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeWRB.json', 'r') as f:\n",
    "    soilTypeWRB = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeQualifiersWRB.json', 'r') as f:\n",
    "    soilTypeQualifiersWRB = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeSpecifiersWRB.json', 'r') as f:\n",
    "    soilTypeSpecifiersWRB = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeUSDA.json', 'r') as f:\n",
    "    soilTypeUSDA = json.load(f)\n",
    "with open(datadir + '../keyword-trees/SoilTypeQualifiersUSDA.json', 'r') as f:\n",
    "    soilTypeQualifiersUSDA = json.load(f)\n",
    "\n",
    "soilvoct = [soilTypeFAO, soilTypeWRB, soilTypeUSDA] # soilTypeGER,\n",
    "soilvocq = [soilTypeQualifiersFAO, soilTypeQualifiersWRB,\n",
    "            soilTypeSpecifiersWRB, soilTypeQualifiersUSDA]\n",
    "soilvoc = []\n",
    "for a in soilvoct:\n",
    "    for b in a['children']:\n",
    "        soilvoc.append(b['value'])\n",
    "        if 'children' in b.keys():\n",
    "            for c in b['children']:\n",
    "                soilvoc.append(c['value'])\n",
    "                if 'children' in c.keys():\n",
    "                    for d in c['children']:\n",
    "                        soilvoc.append(d['value'])\n",
    "soilvoc = [a.lower() for a in soilvoc]\n",
    "soilvoc += [a[:-1] for a in soilvoc if a[-1] == 's']\n",
    "soilvoc.remove('arent')\n",
    "soilvoc.remove('arent')\n",
    "soilvoc.remove('arents')\n",
    "\n",
    "# FAO pattern\n",
    "qualifiers = [dic['value'].strip() for dic in soilTypeQualifiersFAO['children']]\n",
    "soiltypes = [dic['value'].strip() for dic in soilTypeFAO['children']]\n",
    "soiltypes += [a + 's' for a in soiltypes]\n",
    "faoPattern = '({:s})?\\s?({:s})?\\s?({:s})'.format(\n",
    "        '|'.join(qualifiers), '|'.join(qualifiers), '|'.join(soiltypes))\n",
    "\n",
    "# WRB pattern\n",
    "qualifiers = []\n",
    "for dic in soilTypeQualifiersWRB:\n",
    "    qualifiers.extend(dic['principal_qualifiers'])\n",
    "    qualifiers.extend(dic['supplementary_qualifiers'])\n",
    "qualifiers = list(set(qualifiers))\n",
    "qualifiers = [a.strip() for a in qualifiers]\n",
    "soiltypes = [dic['value'].strip() for dic in soilTypeWRB['children']]\n",
    "soiltypes += [a + 's' for a in soiltypes]\n",
    "wrbPattern = '({:s})?\\s?({:s})?\\s?({:s})'.format(\n",
    "    '|'.join(qualifiers), '|'.join(qualifiers), '|'.join(soiltypes))\n",
    "\n",
    "# USDA pattern\n",
    "soiltypes = []\n",
    "for a in soilTypeUSDA['children']:\n",
    "    soiltypes.append(a['value'])\n",
    "    if 'children' in a.keys():\n",
    "        for b in a['children']:\n",
    "            soiltypes.append(b['value'])\n",
    "            if 'children' in b.keys():\n",
    "                for c in b['children']:\n",
    "                    soiltypes.append(c['value'])\n",
    "soiltypes += [a[:-1] for a in soiltypes if a[-1] == 's']\n",
    "qualifiers = [dic['value'] for dic in soilTypeQualifiersUSDA]\n",
    "usdaPattern = '({:s})?\\s?({:s})?\\s?({:s})'.format(\n",
    "    '|'.join(qualifiers), '|'.join(qualifiers), '|'.join(soiltypes))\n",
    "\n",
    "# TODO 'podzolic soil' -> add adjective version of word too for WRB\n",
    "\n",
    "def getSoilType(doc):\n",
    "    text = doc.replace('\\n', ' ')\n",
    "    soilmatches = re.findall(wrbPattern, text)\n",
    "    soilmatches += re.findall(faoPattern, text)\n",
    "    soilmatches += re.findall(usdaPattern, text)\n",
    "    soiltypes = list(set([' '.join(a).replace('  ', ' ').strip() for a in soilmatches]))\n",
    "    return soiltypes\n",
    "\n",
    "# test\n",
    "# getSoilType(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd7d4ce-734b-44e5-a32c-cefe17cfb3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clay', 'clay loam', 'loam', 'sand', 'silt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract soil texture\n",
    "voc = [\n",
    "    'sandy clay loam', 'silty clay loam',\n",
    "    'loamy sand', 'sandy loam', 'clay loam', 'sandy clay',\n",
    "    'silty clay', 'silt loam',\n",
    "    'sand', 'silt', 'clay', 'loam'\n",
    "]\n",
    "    \n",
    "def getSoilTexture(doc):\n",
    "    text = doc.lower()\n",
    "    # remove sand occurence usually applied for better contact with permeameter\n",
    "    text = re.sub('(fine\\s+sand|moist\\s+\\sand|contact\\ssand|sand\\s+pad|washed\\s+sand|(sand|clay|silt)\\s+content)', '', text)\n",
    "    matches = re.findall('|'.join(voc), text)  # maybe case would help here actually\n",
    "    umatches = np.unique(matches).tolist()\n",
    "    return umatches\n",
    "\n",
    "# test\n",
    "# getSoilTexture(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94f8b79-be7f-409c-8cdd-1a7cae1bf680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['855']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract rainfall\n",
    "def getRainfall(doc):\n",
    "    text = doc.replace('\\n',' ').lower()\n",
    "    text = re.sub('\\([a-z\\s]+\\., \\d{4}\\)', '', text)  # remove citation in parenthesis \n",
    "    text = text = re.sub('[^0-9a-z.,\\s\\-–]', '', text)  # remove other characters\n",
    "    matches = re.findall('((?:cumulated|annual|average)[a-z\\s]+(?:rainfall|rain|precipitation))(?:[a-z\\s]+)?(\\d+[.-\\–]?\\d+c)?[a-z\\s]+(\\d+\\.?,?\\d+(?:-|–|\\sand\\s|\\sto\\s)?(?:\\d+)?)\\s?(m\\s?m|cm)', text)\n",
    "    umatches = []\n",
    "    for a in matches:\n",
    "        if a[-1] == 'mm':\n",
    "            s = a[-2]\n",
    "        elif a[-1] == 'cm':\n",
    "            s = '{:.0f}'.format(float(a[-2])*10)\n",
    "        if ('-' in s) or ('–' in s) or ('to' in s):\n",
    "            s = '{:.0f}'.format(np.mean([float(a) for a in re.split('-|–|to', s)]))\n",
    "        s = s.replace(',', '')\n",
    "        if 'and' in s:\n",
    "            umatches.append(s.split(' and ')[0])\n",
    "            umatches.append(s.split(' and ')[1])\n",
    "        else:\n",
    "            umatches.append(s)\n",
    "    return umatches\n",
    "\n",
    "# test\n",
    "# getRainfall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "865a8dec-c030-4125-819d-40ada5877026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract altitude/elevation\n",
    "def getElevation(text):\n",
    "    match1 = re.findall('((\\d+)\\s?m[a-z\\s]+(altitude|elevation))', text.replace('\\n',' ').lower())\n",
    "    match2 = re.findall('((altitude|elevation)[a-z\\s]+(\\d+)\\s?m)', text.replace('\\n',' ').lower())\n",
    "    match = match1 + match2\n",
    "    doc = text.replace('\\n', ' ').lower()\n",
    "    matchd = [doc[m.start()-20:m.end()+20] for m in re.finditer('(altitude|elevation)', doc)]\n",
    "    return matchd\n",
    "\n",
    "# test\n",
    "# getElevation(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72793f-f261-498e-a723-8ef1bd1d44d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract diameter\n",
    "def getDiameter(doc):\n",
    "    text = doc.replace('\\n',' ').lower()\n",
    "    match1 = re.findall('(radius|diameter)[a-z\\s]+(\\d+\\.?\\d+)\\s?(cm|mm)', text)\n",
    "    match2 = re.findall('(\\d+\\.?\\d+)\\s?(cm|mm)[a-z\\s]+(radius|diameter)', text)\n",
    "    match = match1 + match2\n",
    "    umatches = []\n",
    "    for a in match1:\n",
    "        val = float(a[1])\n",
    "        val = val/10 if a[2] == 'mm' else val\n",
    "        val = val * 2 if a[0] == 'radius' else val\n",
    "        umatches.append(val)\n",
    "    for a in match2:\n",
    "        val = float(a[0])\n",
    "        val = val/10 if a[1] == 'mm' else val\n",
    "        val = val * 2 if a[2] == 'radius' else val\n",
    "        umatches.append(val)\n",
    "    return umatches\n",
    "\n",
    "# test\n",
    "# getDiameter(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12051d-ef70-4bb9-a9f6-53d09750b997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120.0, 60.0, 30.0, 10.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract tensions\n",
    "def getTensions(doc):\n",
    "    text = doc.replace('\\n', ' ').lower()\n",
    "    #matches = re.findall('(\\-?\\s?\\d{1,3},\\s)+\\s+and\\s+(\\-?\\s?\\d{1,3}\\s(mm|cm))', text)\n",
    "    text = text.replace(' and ', ', ')\n",
    "    matches = re.findall('((?:(?:-?\\d+),?\\s){2,})\\s?(mm|cm)', text)\n",
    "    tt = []\n",
    "    if len(matches) > 0:\n",
    "        if ',' in matches[0][0]:\n",
    "            tt = np.abs([float(a.strip()) for a in matches[0][0].split(',')]).tolist()\n",
    "            if matches[0][-1] == 'cm':\n",
    "                tt = [10*t for t in tt]\n",
    "            tmin = np.min(tt)\n",
    "            tmax = np.max(tt)\n",
    "    return tt\n",
    "\n",
    "# test\n",
    "# getTensions(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eea44d-8405-4345-9ef6-6ad9186b1cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>meaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEST</td>\n",
       "      <td>of Soil Transfer parameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PI</td>\n",
       "      <td>pressure inﬁltrometer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SFH</td>\n",
       "      <td>simpliﬁed falling head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TI</td>\n",
       "      <td>tension inﬁltrometer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDI</td>\n",
       "      <td>mini disk inﬁltrometer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BB</td>\n",
       "      <td>bottomless bucket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PSD</td>\n",
       "      <td>particle size distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TPD</td>\n",
       "      <td>with the TwoPonding-Depth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abbreviation                       meaning\n",
       "0  BEST         of Soil Transfer parameters \n",
       "1  PI           pressure inﬁltrometer       \n",
       "2  SFH          simpliﬁed falling head      \n",
       "3  TI           tension inﬁltrometer        \n",
       "4  MDI          mini disk inﬁltrometer      \n",
       "5  BB           bottomless bucket           \n",
       "6  PSD          particle size distribution  \n",
       "7  TPD          with the TwoPonding-Depth   "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract abbreviations\n",
    "def getAbbreviations(text):\n",
    "    dfabb = pd.DataFrame(columns=['abbreviation', 'meaning'])\n",
    "    abrvs = [a for a in re.findall(r'(\\([A-Z]*\\))', text) if len(a) > 3]\n",
    "    for abrv in abrvs:\n",
    "        meaning = ' '.join(re.split(' |\\\\n', text.split(abrv)[0])[-(len(abrv)-1):])\n",
    "        dfabb = dfabb.append({'abbreviation': abrv[1:-1],\n",
    "                              'meaning': meaning}, ignore_index=True)\n",
    "    dfabb = dfabb.drop_duplicates().reset_index(drop=True)\n",
    "    return dfabb\n",
    "\n",
    "# test\n",
    "# getAbbreviations(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b16ce-9bab-4797-8643-43f92883cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(38.10694, 13.35167)]\n"
     ]
    }
   ],
   "source": [
    "# extract coordinates\n",
    "\n",
    "# convert to decimal degree\n",
    "def dms2dec(out):\n",
    "    # string, deg, symbol, minute, symbol, second, symbol, direction\n",
    "    deg = float(out[1]) if out[1] != '' else 0\n",
    "    mnt = float(out[3]) if out[3] != '' else 0\n",
    "    sec = float(out[5]) if out[5] != '' else 0\n",
    "    dec = deg + mnt/60 + sec/60/60\n",
    "    dec = -dec if (out[-1] == 'S')|(out[-1] == 'W')|(out[-1] == 'O') else dec\n",
    "    return dec\n",
    "\n",
    "def getCoordinates(text):\n",
    "    text = text.replace('\\n',' ').replace('\\x04','*').replace('\\x03','*')\n",
    "    text = text.replace('\\x01','*').replace(',','.') # \\x04 is EOT End Of Transmission\n",
    "    latTime = re.findall(\"(([+-]?[1-8]?\\d|[+-]?90)([°◦*oO])\\s?(\\d{1,2})(.)\\s?(\\d{1,2}(\\.\\d+)?)?.?.?\\s?(latitude\\s)?([NS]))\", text)\n",
    "    lonTime = re.findall(\"(([+-]?180|[+-]?1[0-7]\\d|[+-]?[1-9]?\\d)([°◦*oO])\\s?(\\d{1,2}?)(.)\\s?(\\d{1,2}(\\.\\d+)?)?.?.?\\s?(longitude\\s)?([WEO]))\", text)\n",
    "\n",
    "    # sometimes the symbol for minutes is transformed in a zero\n",
    "    if len(latTime) > 0:\n",
    "        for l in range(len(latTime)):\n",
    "            a = latTime[l]\n",
    "            if a[3] == ' ':  # we would expect the symbol for minute\n",
    "                latTime[l] = a[:2] + (latTime[l][2][:-1],) + a[3:]  # remove last symbol\n",
    "    if len(lonTime) > 0:\n",
    "        for l in range(len(lonTime)):\n",
    "            a = lonTime[l]\n",
    "            if a[3] == ' ':  # we would expect the symbol for minute\n",
    "                lonTime[l] = a[:2] + (lonTime[l][2][:-1],) + a[3:]  # remove last symbol\n",
    "\n",
    "    # try some popular edge cases\n",
    "    if len(latTime) == 0:\n",
    "        latTime = re.findall(\"((\\d+)(8)(\\d{2,}))(['0V9])\\s?(\\d+\\.\\d+)?()\\s?(latitude\\s)?([NS])\", text)\n",
    "        lonTime = re.findall(\"((\\d+)(8)(\\d{2,}))(['0V9])\\s?(\\d+\\.\\d+)?()\\s?(longitude\\s)?([WOE])\", text)\n",
    "        \n",
    "    if len(latTime) == 0:\n",
    "        latTime = re.findall(\"(([NS])\\s(\\d+)(8)(\\d{2,}))(['0V9])\\s?(\\d+\\.\\d+)?()\", text)\n",
    "        lonTime = re.findall(\"(([WOE])\\s(\\d+)(8)(\\d{2,}))(['0V9])\\s?(\\d+\\.\\d+)?()\", text)\n",
    "        latTime = [a[1:] + (a[-1],) for a in latTime]\n",
    "        lonTime = [a[1:] + (a[-1],) for a in lonTime]\n",
    "\n",
    "    # convert to decimal degree\n",
    "    lats = [dms2dec(a) for a in latTime]\n",
    "    lons = [dms2dec(a) for a in lonTime]\n",
    "\n",
    "    # try to match decimal notation\n",
    "    if len(lats) == 0:\n",
    "        latDeg = re.findall(\"([+-]?(([1-8]?\\d|90)\\.\\d+)[°◦*oO⬚]?\\s?([NS]))\", text)\n",
    "        lonDeg = re.findall(\"([-+]?((180|1[0-7]\\d|[1-9]?\\d)\\.\\d+)[°◦*oO⬚]?\\s?([WEO]))\", text)\n",
    "        lats = [float(a[1]) if a[3] == 'N' else -float(a[1]) for a in latDeg]\n",
    "        lons = [float(a[1]) if a[3] == 'E' else -float(a[1]) for a in lonDeg]\n",
    "\n",
    "    # edge case of decimal notation\n",
    "    if len(lats) == 0:\n",
    "        latDeg = re.findall(\"(lat\\.\\s.(([+-]?[1-8]?\\d|[+-]?90)\\.\\d+)[°◦*oO⬚]?)\", text)\n",
    "        lonDeg = re.findall(\"(long\\.\\s.(([-+]?180|[-+]?1[0-7]\\d|[-+]?[1-9]?\\d)\\.\\d+)[°◦*oO⬚]?)\", text)\n",
    "        lats = [float(a[1]) for a in latDeg]\n",
    "        lons = [float(a[1]) for a in lonDeg]\n",
    "        \n",
    "    # rounding for easier comparison\n",
    "    lats = [np.round(a, 5) for a in lats]\n",
    "    lons = [np.round(a, 5) for a in lons]\n",
    "    \n",
    "    return [(a, b) for a, b in zip(lats, lons)]\n",
    "\n",
    "# show map\n",
    "def showMap(coords, out):\n",
    "    m = Map(center=(0,0), zoom=2)#center=(52.204793, 360.121558), zoom=15)\n",
    "    for coord in coords:\n",
    "        marker = Marker(location=coord)\n",
    "        m.add_layer(marker)\n",
    "    with out:\n",
    "        display(m)\n",
    "\n",
    "# test\n",
    "# coords = getCoordinates(doc)\n",
    "# print(coords)\n",
    "# out = widgets.Output()\n",
    "# showMap(coords, out)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a634090-6318-4b01-8312-07f90c1119ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>practice</th>\n",
       "      <th>occurence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crop</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  practice occurence\n",
       "0  crop     4       "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract other practices information\n",
    "dfsyn = pd.read_csv(datadir + '../dfsyn.csv')  # load original words and their synonyms\n",
    "\n",
    "def getPractices(doc):\n",
    "    dfpra = pd.DataFrame(columns=['practice', 'occurence'])\n",
    "    lemmas = dfsyn['lemma'].unique()\n",
    "    for lemma in lemmas:\n",
    "        matches = re.findall(lemma, doc.lower())\n",
    "        if len(matches) > 0:\n",
    "            dfpra = dfpra.append({'practice': lemma, 'occurence': len(matches)}, ignore_index=True)\n",
    "    dfpra = dfpra.sort_values('occurence', ascending=False).reset_index(drop=True)\n",
    "    return dfpra\n",
    "\n",
    "# test\n",
    "# getPractices(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b971c7ca-96c5-4f6b-8428-94e289291768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could add automatic DOI detection and cross-ref info? but not really NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58799f88-6b34-431e-83ce-054e6e2d8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your new extraction function\n",
    "def getInfo(text):\n",
    "    info = re.findall('\\d+', text)[0]\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "321d5770-5c8c-46a3-bb4b-fd3cfc178364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDownload(df, name='Download File', fname='df.csv'):\n",
    "    t0 = time.time()\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        fpath = os.path.join(td + fname)\n",
    "        df.to_csv(fpath, index=False)  # takes some time but csv takes more time to render in base64\n",
    "        #with ZipFile(fpath.replace('.csv', '.zip'), mode='w') as myzip:\n",
    "        #    myzip.write(fpath, arcname=fname.replace('.csv', ''))\n",
    "        with open(fpath,  'rb') as f:\n",
    "            data = f.read()\n",
    "    b64 = base64.b64encode(data)\n",
    "    payload = b64.decode()\n",
    "\n",
    "    html_button = '''\n",
    "    <a download=\"{fname}\" href=\"data:text/csv;base64,{payload}\" download>\n",
    "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-info\">{name}</button>\n",
    "    </a>\n",
    "    '''.format(payload=payload, fname=fname, name=name)\n",
    "\n",
    "    return HTML(html_button)\n",
    "\n",
    "#DownloadDownload(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c25226a-ec56-4247-bf9e-50ec3bf31b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata\n",
    "def getMetaData(fo):\n",
    "    data = {}\n",
    "    sdata = []  # possibly get a table with two columns: metadata and values\n",
    "    # not stacked in cell (but then problem with mixed type)\n",
    "    text = pdf2text(fo)\n",
    "    doc, abstract = getAbstract(text)\n",
    "    metadata = {\n",
    "        'Soil type (WRB/USDA)': getSoilType,\n",
    "        'Soil texture (USDA)': getSoilTexture,\n",
    "        'Disk diameter [mm]': getDiameter,\n",
    "        'Tensions [mm]': getTensions,\n",
    "        'Elevation [masl]': getElevation,\n",
    "        'Rainfall [mm/year]': getRainfall,\n",
    "        'Coordinates [degree]': getCoordinates,\n",
    "        # add new function here\n",
    "    }\n",
    "    for col in metadata:\n",
    "        func = metadata[col]\n",
    "        res = func(doc)  # apply the function to the document\n",
    "        out = ', '.join(str(a) for a in res)\n",
    "        if col == 'Coordinates [degree]':\n",
    "            coords = out\n",
    "            out = '; '.join(['({:.5f}, {:.5f})'.format(a, b) for a, b in res])\n",
    "            data['Latitude [degree]'] = ', '.join(['{:.5f}'.format(a[0]) for a in res])\n",
    "            data['Longitude [degree]'] = ', '.join(['{:.5f}'.format(a[1]) for a in res])\n",
    "            for a, b in res:\n",
    "                sdata.append(['Latitude [degree]', a])\n",
    "                sdata.append(['Longitude [degree]', b])\n",
    "                sdata.append([col, '({:.5f}, {:.5f})'.format(a, b)])\n",
    "        else:\n",
    "            data[col] = [out]\n",
    "            for a in res:\n",
    "                sdata.append([col, a])\n",
    "    #dfmeta = pd.DataFrame(data)\n",
    "    dfmeta = pd.DataFrame(sdata, columns=['metadata', 'value'])\n",
    "    dfabb = getAbbreviations(doc)\n",
    "    dfpra = getPractices(doc)\n",
    "    return dfmeta, dfabb, dfpra, abstract, coords\n",
    "\n",
    "# test\n",
    "# with open('../data/from-kunsat/papers/alagna2016.pdf', 'rb') as f:\n",
    "#     fo = f.read()\n",
    "# getMetaData(fo)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7961cf5-7a4f-45e5-80a8-3eda29cc2f1e",
   "metadata": {},
   "source": [
    "- download all tables as .csv (using the html data tab)\n",
    "- download all converted text (as zip)\n",
    "- drop-down menu to change which pdf to display\n",
    "- nice way of adding a new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8787f2a7-94c8-4687-a552-fa60e0e294a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown.unobserve(showDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad00b9e8-2956-4483-8df5-63594fe4fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c194190d1b450da19a8c5d7a344b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value={}, description='Upload pdf', multiple=True), Output(), Dropdown(description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# interface\n",
    "\n",
    "# create global variables\n",
    "dfmetas = pd.DataFrame()\n",
    "dfabbs = pd.DataFrame()\n",
    "dfpras = pd.DataFrame()\n",
    "dfabstracts = pd.DataFrame()\n",
    "\n",
    "# function automatically called on upload for processing pdfs\n",
    "def uploadBtnFunc(_):\n",
    "    global dfmetas, dfabbs, dfpras, dfabstracts\n",
    "    download.clear_output()\n",
    "    output.clear_output()\n",
    "    dropdown.unobserve(showDocument)    \n",
    "    with download:\n",
    "        t0 = time.time()\n",
    "        print('processing...', end='')\n",
    "    if len(uploadBtn.data) > 0:\n",
    "        options = []\n",
    "        n = len(uploadBtn.data)\n",
    "        dfmetas, dfabbs, dfpras, abstracts = [], [], [], []\n",
    "        fname0 = ''\n",
    "        for i in range(n):\n",
    "            if uploadBtn.metadata[i]['type'] == 'application/pdf':\n",
    "                fname = uploadBtn.metadata[i]['name']\n",
    "                if fname0 == '':\n",
    "                    fname0 = fname\n",
    "                dfmeta, dfabb, dfpra, abstract, coords = getMetaData(uploadBtn.data[i])\n",
    "                dfmeta['fname'] = fname\n",
    "                dfmetas.append(dfmeta)\n",
    "                dfabb['fname'] = fname\n",
    "                dfabbs.append(dfabb)\n",
    "                dfpra['fname'] = fname\n",
    "                dfpras.append(dfpra)\n",
    "                abstracts.append([fname, abstract])\n",
    "                with download:\n",
    "                    print('\\rprocessing...', i+1, '/', n, end='')\n",
    "                options.append(fname)\n",
    "            else:\n",
    "                print('only PDF file accepted')\n",
    "        dropdown.options = options\n",
    "\n",
    "        # create large dataframe\n",
    "        dfmetas = pd.concat(dfmetas, axis=0)\n",
    "        dfabbs = pd.concat(dfabbs, axis=0)\n",
    "        dfpras = pd.concat(dfpras, axis=0)\n",
    "        dfabstracts = pd.DataFrame(abstracts, columns=['fname', 'abstract'])\n",
    "        \n",
    "        with download:\n",
    "            display(prepareDownload(dfmetas, 'Metadata', 'dfmetadata.csv'))\n",
    "            display(prepareDownload(dfabbs, 'Abbreviations', 'dfabbreviations.csv'))\n",
    "            display(prepareDownload(dfpras, 'Practices', 'dfpractices.csv'))\n",
    "            display(prepareDownload(dfabstracts, 'Abstracts', 'dfabstracts.csv'))\n",
    "            print('done ({:.2f}s)'.format(time.time() - t0))\n",
    "\n",
    "        dropdown.observe(showDocument, names='value')\n",
    "        showDocument({'new': fname0})\n",
    "        uploadBtn.value.clear()\n",
    "\n",
    "def showDocument(a):\n",
    "    global dfmetas, dfabbs, dfpras, dfabstracts\n",
    "    fname = a['new']\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        display(dfmetas[dfmetas['fname'] == fname])\n",
    "        display(dfabbs[dfabbs['fname'] == fname])\n",
    "        display(dfpras[dfpras['fname'] == fname])\n",
    "        lats = dfmetas[dfmetas['fname'].eq(fname) & dfmetas['metadata'].eq('Latitude [degree]')]['value']\n",
    "        lons = dfmetas[dfmetas['fname'].eq(fname) & dfmetas['metadata'].eq('Latitude [degree]')]['value']\n",
    "        coords = [(float(a), float(b)) for a, b in zip(lats, lons)]\n",
    "        showMap(coords, output)\n",
    "        print('ABSTRACT:')\n",
    "        print(dfabstracts[dfabstracts['fname'] == fname]['abstract'])\n",
    "        \n",
    "uploadBtn = widgets.FileUpload(description='Upload pdf', multiple=True)\n",
    "uploadBtn.observe(uploadBtnFunc, names='value')\n",
    "\n",
    "dropdown = widgets.Dropdown(description='Show')\n",
    "\n",
    "download = widgets.Output()\n",
    "output = widgets.Output()\n",
    "\n",
    "# layout\n",
    "widgets.VBox([uploadBtn, download,  dropdown, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "09120ac4-766b-4d45-b5be-e1af53484ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alagna2016.pdf\n"
     ]
    }
   ],
   "source": [
    "showDocument({'new':'alagna2016.pdf'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "752cab86-ca50-4e31-a57a-fcf8f502eb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>value</th>\n",
       "      <th>fname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Soil texture (USDA)</td>\n",
       "      <td>clay</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Soil texture (USDA)</td>\n",
       "      <td>clay loam</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soil texture (USDA)</td>\n",
       "      <td>loam</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soil texture (USDA)</td>\n",
       "      <td>sand</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soil texture (USDA)</td>\n",
       "      <td>silt</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tensions [mm]</td>\n",
       "      <td>120.0</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tensions [mm]</td>\n",
       "      <td>60.0</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tensions [mm]</td>\n",
       "      <td>30.0</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tensions [mm]</td>\n",
       "      <td>10.0</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rainfall [mm/year]</td>\n",
       "      <td>855</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Latitude [degree]</td>\n",
       "      <td>38.10694</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Longitude [degree]</td>\n",
       "      <td>13.35167</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Coordinates [degree]</td>\n",
       "      <td>(38.10694, 13.35167)</td>\n",
       "      <td>alagna2016.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                metadata                 value           fname\n",
       "0   Soil texture (USDA)   clay                  alagna2016.pdf\n",
       "1   Soil texture (USDA)   clay loam             alagna2016.pdf\n",
       "2   Soil texture (USDA)   loam                  alagna2016.pdf\n",
       "3   Soil texture (USDA)   sand                  alagna2016.pdf\n",
       "4   Soil texture (USDA)   silt                  alagna2016.pdf\n",
       "5   Tensions [mm]         120.0                 alagna2016.pdf\n",
       "6   Tensions [mm]         60.0                  alagna2016.pdf\n",
       "7   Tensions [mm]         30.0                  alagna2016.pdf\n",
       "8   Tensions [mm]         10.0                  alagna2016.pdf\n",
       "9   Rainfall [mm/year]    855                   alagna2016.pdf\n",
       "10  Latitude [degree]     38.10694              alagna2016.pdf\n",
       "11  Longitude [degree]    13.35167              alagna2016.pdf\n",
       "12  Coordinates [degree]  (38.10694, 13.35167)  alagna2016.pdf"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmetas[dfmetas['fname'].eq('alagna2016.pdf')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
